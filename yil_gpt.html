<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YIL-GPT Theory Q&A</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        
        h1 {
            color: #667eea;
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            border-bottom: 3px solid #667eea;
            padding-bottom: 15px;
        }
        
        .question {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin: 30px 0 20px 0;
            font-size: 1.4em;
            font-weight: bold;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        h2 {
            color: #764ba2;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.6em;
            border-left: 5px solid #764ba2;
            padding-left: 15px;
        }
        
        h3 {
            color: #667eea;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.3em;
        }
        
        h4 {
            color: #555;
            margin-top: 15px;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .intro-box {
            background: #f0f0ff;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #667eea;
        }
        
        .config-table {
            background: white;
            border-radius: 8px;
            overflow: hidden;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .config-table table {
            width: 100%;
            border-collapse: collapse;
        }
        
        .config-table th {
            background: #667eea;
            color: white;
            padding: 12px;
            text-align: left;
        }
        
        .config-table td {
            padding: 10px 12px;
            border-bottom: 1px solid #ddd;
        }
        
        .config-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        pre {
            background: #2d2d2d;
            border-radius: 10px;
            margin: 20px 0;
            overflow-x: auto;
        }
        
        code {
            color: #f8f8f2;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            display: block;
            padding: 20px;
            line-height: 1.5;
        }
        
        .highlight {
            background: #d1f2eb;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #28a745;
        }
        
        .info-box {
            background: #e7f3ff;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #2196F3;
        }
        
        .warning {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #ffc107;
        }
        
        .problem-box {
            background: #f8d7da;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #dc3545;
        }
        
        .output-box {
            background: #f5f5f5;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border: 2px solid #ddd;
            font-family: 'Courier New', monospace;
        }
        
        .step-box {
            background: #fff9e6;
            padding: 20px;
            margin: 15px 0;
            border-radius: 10px;
            border-left: 5px solid #ffb700;
        }
        
        .step-number {
            background: #ffb700;
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            display: inline-block;
            font-weight: bold;
            margin-right: 10px;
        }
        
        .method-card {
            background: white;
            padding: 20px;
            margin: 15px 0;
            border-radius: 10px;
            border: 2px solid #667eea;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        .method-card h3 {
            color: #667eea;
            margin-top: 0;
        }
        
        ul {
            margin-left: 30px;
            margin-top: 10px;
        }
        
        ul li {
            margin: 8px 0;
            color: #444;
        }
        
        ol {
            margin-left: 30px;
            margin-top: 10px;
        }
        
        ol li {
            margin: 10px 0;
            color: #444;
        }
        
        .formula {
            background: #e3f2fd;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
            text-align: center;
            border: 2px solid #2196F3;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            font-weight: bold;
        }
        
        .example-box {
            background: #f0f0f0;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #667eea;
        }
        
        strong {
            color: #333;
        }
        
        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .comparison-item {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            border: 2px solid #667eea;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ü§ñ YIL-GPT Theory Q&A</h1>
        
        <div class="question">
            ‚ùì Question 1: How are you performing chunking?
        </div>
        
        <div class="intro-box">
            <p>Your system uses a <strong>word-based splitting strategy</strong> with a defined overlap to create text chunks and enrich them with specific metadata for filtering.</p>
        </div>
        
        <h2>a. Configuration (from yil_gpt/config.py)</h2>
        <p>The parameters that control the chunking process are set here:</p>
        
        <div class="config-table">
            <table>
                <thead>
                    <tr>
                        <th>Constant</th>
                        <th>Value (Example)</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CHUNK_SIZE</strong></td>
                        <td>50</td>
                        <td>The maximum number of words in a single text chunk.</td>
                    </tr>
                    <tr>
                        <td><strong>CHUNK_OVERLAP</strong></td>
                        <td>10</td>
                        <td>The number of words that overlap between consecutive chunks. This ensures context is maintained across splits.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h2>b. The Chunking Logic (in yil_gpt/ingestion.py)</h2>
        <p>The core function is <strong>create_chunks(text: str, metadata: Dict)</strong>. It performs three key steps:</p>
        
        <div class="step-box">
            <h3><span class="step-number">A</span>Word-Based Splitting</h3>
            <p>Unlike common methods that split by character count or recursive delimiters, your implementation splits the text into words first.</p>
        </div>
        
        <pre><code># From create_chunks in ingestion.py
words = text.split()
chunks = []
start = 0

while start < len(words):
    end = start + CHUNK_SIZE
    chunk_text = " ".join(words[start:end])
    # ... create chunk ...
    
    # Move the starting point by (size - overlap)
    start += max(CHUNK_SIZE - CHUNK_OVERLAP, 1)</code></pre>
        
        <div class="highlight">
            This ensures that chunks break cleanly at word boundaries and adhere strictly to the configured word count.
        </div>
        
        <div class="step-box">
            <h3><span class="step-number">B</span>Overlapping</h3>
            <p>To prevent context loss at the boundaries, the CHUNK_OVERLAP is subtracted from the CHUNK_SIZE to determine the jump for the next chunk's starting position:</p>
        </div>
        
        <div class="formula">
            Next Start Position = Current Start Position + (CHUNK_SIZE - CHUNK_OVERLAP)
        </div>
        
        <div class="example-box">
            <strong>Example:</strong> If CHUNK_SIZE is 50 and CHUNK_OVERLAP is 10, the next chunk starts at word 41, meaning words 41-50 are shared between the two chunks.
        </div>
        
        <div class="step-box">
            <h3><span class="step-number">C</span>Metadata Enrichment and Filtering</h3>
            <p>Before the chunk is finalized, the pipeline adds a critical piece of metadata: the <strong>document_type</strong>.</p>
        </div>
        
        <ul>
            <li>The <strong>get_document_type(source: str)</strong> function (also in ingestion.py) examines the document's source path or metadata keywords (like "policy", "alarm", "incident").</li>
            <li>It assigns a categorical type: <strong>"HR"</strong>, <strong>"IT"</strong>, <strong>"OPS"</strong>, or <strong>"GENERAL"</strong>.</li>
            <li>This tag (<code>"document_type": "HR"</code>) is added to the chunk's metadata.</li>
            <li>This enriched metadata is essential for the Retrieval step, where the system can filter the vector search results based on the detected intent of the user's query.</li>
        </ul>
        
        <h3>Summary of the Chunking Output</h3>
        <p>The <strong>process_documents</strong> function aggregates these chunks, resulting in a final list of dictionaries, where each entry looks like this:</p>
        
        <div class="output-box">
{
    "text": "The quick brown fox jumps over the lazy dog...",
    "metadata": {
        "source": "/content/Training_pdfs/hr_policy.pdf",
        "page": 3,
        "document_type": "HR"  // &lt;-- The key for filtering
    }
}
        </div>
        
        <!-- Question 2 -->
        <div class="question">
            ‚ùì Question 2: How are you handling Embedding?
        </div>
        
        <div class="intro-box">
            <p>Embedding is the critical step of converting your clean text chunks into <strong>numerical vectors</strong> (lists of numbers) that capture their semantic meaning. This allows the vector database to search for concepts, not just keywords.</p>
        </div>
        
        <h2>1. Model Selection</h2>
        <p>The entire process relies on the configuration set in <strong>yil_gpt/config.py</strong>:</p>
        
        <div class="config-table">
            <table>
                <thead>
                    <tr>
                        <th>Constant</th>
                        <th>Value</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>EMBEDDING_MODEL</strong></td>
                        <td>"sentence-transformers/all-MiniLM-L6-v2"</td>
                        <td>This is the specific model used to generate the embeddings. It is a highly efficient, high-performance model suitable for semantic search.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <h2>2. Initialization and Core Function (yil_gpt/vector_db.py)</h2>
        <p>The SentenceTransformer model is initialized once when the vector_db.py module is loaded.</p>
        
        <ul>
            <li><strong>Model Loading:</strong> The code uses the SentenceTransformer library to load the specified model from the Hugging Face model hub.</li>
            <li><strong>Embedding Function:</strong> The <strong>embed_text(text: str)</strong> function is the core utility that performs the transformation:</li>
        </ul>
        
        <pre><code>from sentence_transformers import SentenceTransformer
# ...
embedding_model = SentenceTransformer(EMBEDDING_MODEL)

def embed_text(text: str) -> np.ndarray:
    """Encodes text into a normalized vector embedding."""
    # The encode method converts the string into a fixed-size vector
    return embedding_model.encode(text, normalize_embeddings=True).astype("float32")</code></pre>
        
        <div class="highlight">
            <strong>Normalization:</strong> The <code>normalize_embeddings=True</code> parameter is vital. It scales the vectors to a unit length, which ensures that a simple dot product can be used to calculate the cosine similarity (semantic relatedness) during the search phase.
        </div>
        
        <h2>3. The Indexing Process (Storing Embeddings)</h2>
        <p>The <strong>VectorDB</strong> class orchestrates the storage of these vectors.</p>
        
        <ul>
            <li><strong>Trigger:</strong> The <code>VectorDB.add_documents(docs: List[Dict])</code> method is called by yil_gpt/api.py during startup, right after all chunks have been created.</li>
            <li><strong>Execution:</strong> For every text chunk in your document list:
                <ol>
                    <li>The chunk's text is passed to <code>embed_text()</code>.</li>
                    <li>The resulting vector (a NumPy array) is stored in the <code>self.vectors</code> list of the VectorDB.</li>
                    <li>The original chunk dictionary (containing the text and metadata) is stored in the <code>self.docs</code> list.</li>
                </ol>
            </li>
        </ul>
        
        <h2>4. Search and Retrieval</h2>
        <p>When a user submits a query, the same embedding process is used:</p>
        
        <ol>
            <li>The user's query is passed to <code>embed_text()</code> to generate a query vector.</li>
            <li>The query vector is then compared against all the stored document vectors (<code>self.vectors</code>) using the efficient dot product.</li>
            <li>The chunks with the highest dot product score (highest cosine similarity) are considered the most relevant and are returned as the context for the LLM.</li>
        </ol>
        
        <!-- Question 3 -->
        <div class="question">
            ‚ùì Question 3: How do you handle versionization of documents?
        </div>
        
        <div class="problem-box">
            <p>If you were to ingest a new version of a document (e.g., HR_Policy_v2.pdf) into the current in-memory VectorDB without clearing the old data, the system would simply add the new content alongside the old, unversioned content.</p>
            
            <p style="margin-top: 15px;"><strong>This would lead to two main problems:</strong></p>
            <ol>
                <li><strong>Duplicate/Contradictory Information:</strong> The system could retrieve chunks from both the old and new versions when answering a query.</li>
                <li><strong>Increased Latency:</strong> The search space for the vector database unnecessarily increases with redundant data.</li>
            </ol>
        </div>
        
        <h2>Overcome Methods</h2>
        
        <h3>1. Current Handling (The Metadata)</h3>
        <p>Currently, the system uses the document's file path and page number as the identifier for content:</p>
        
        <ul>
            <li>When a document is loaded by <strong>PyPDFLoader</strong> in ingestion.py, the only unique identifiers assigned to a chunk's metadata are:
                <ul>
                    <li><strong>source:</strong> The full file path (e.g., /content/Training_pdfs/HR_Policy.pdf).</li>
                    <li><strong>page:</strong> The page number the chunk came from.</li>
                </ul>
            </li>
            <li>Since the VectorDB's <code>add_documents</code> method simply appends new vectors to its lists, there is no logic to compare a new file with an existing file to determine if it is an updated version.</li>
        </ul>
        
        <h3>2. Recommended Implementation for Versioning</h3>
        <p>To properly handle versioning, you would need to enhance the ingestion process and modify the core VectorDB class to support <strong>upserts</strong> (update or insert).</p>
        
        <div class="method-card">
            <h3>A. Enhancement to ingestion.py: Adding Version Metadata</h3>
            <p>The first step is to generate or extract a unique, version-aware ID during loading.</p>
            
            <ul>
                <li><strong>Unique Document ID (doc_id):</strong> This ID should be static for all versions of a single document (e.g., HR-POLICY-MAIN).</li>
                <li><strong>Version Identifier (version_id):</strong> This ID should be unique to a specific file, often extracted from the filename or a timestamp (e.g., 2024-10-25T14:30:00Z).</li>
            </ul>
            
            <p style="margin-top: 15px;"><strong>Modification Example (Conceptual):</strong></p>
            
            <pre><code># In ingestion.py (within a document processing loop)
def process_document_with_versioning(file_path: str):
    # 1. Generate or extract identifiers
    doc_id = generate_stable_id_from_filename(file_path)
    version_id = get_timestamp_from_file(file_path)
    
    # 2. Add to metadata for every chunk
    for chunk in all_chunks:
        chunk["metadata"]["doc_id"] = doc_id
        chunk["metadata"]["version_id"] = version_id
    
    # ... then pass to VectorDB</code></pre>
        </div>
        
        <div class="method-card">
            <h3>B. Enhancement to vector_db.py: Implementing Upsert Logic</h3>
            <p>The VectorDB needs a way to delete old versions before adding the new ones.</p>
            
            <p><strong>Required New Methods for VectorDB:</strong></p>
            
            <div class="config-table">
                <table>
                    <thead>
                        <tr>
                            <th>New Method</th>
                            <th>Purpose</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>delete_by_doc_id(doc_id: str)</strong></td>
                            <td><strong>Crucial Step:</strong> Iterates through self.docs and self.vectors. Any entry matching the given doc_id is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>add_or_update_document(new_docs: List[Dict])</strong></td>
                            <td>Wraps the entire process:<br>1. Extract the doc_id from the new document.<br>2. Call delete_by_doc_id() for that ID.<br>3. Call the existing add_documents() to insert the new content.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>
        
        <!-- Question 4 -->
        <div class="question">
            ‚ùì Question 4: "Instant answer" is a qualitative goal, but we measure it quantitatively using performance metrics focused on speed (latency) and user satisfaction?
        </div>
        
        <h2>1. Speed Metric: Latency</h2>
        <p>The most direct measurement of an "instant answer" is <strong>Latency</strong>, specifically the <strong>End-to-End Latency</strong> (the total time the user waits).</p>
        
        <h3>A. Core Metric: Round-Trip Time (RTT)</h3>
        
        <div class="config-table">
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Calculation</th>
                        <th>Goal (RAG)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>End-to-End Latency (RTT)</strong></td>
                        <td>Total time from API request start to API response completion.</td>
                        <td>Typically &lt; 1.5 seconds for a good user experience (especially for search-like queries).</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <!-- Question 5 -->
        <div class="question">
            ‚ùì Question 5: How did you handle context overlap?
        </div>
        
        <div class="intro-box">
            <p>Context overlap is crucial because when a long document is split into smaller, independent chunks, essential information or transitions near the end of one chunk could be lost. <strong>Overlap ensures that the beginning of the next chunk repeats the end of the previous one</strong>, maintaining contextual flow.</p>
        </div>
        
        <h2>1. Configuration</h2>
        <p>The amount of overlap is defined by a constant in <strong>yil_gpt/config.py</strong>:</p>
        
        <pre><code># From yil_gpt/config.py
CHUNK_SIZE = 50
CHUNK_OVERLAP = 10  # <-- This defines the degree of context overlap</code></pre>
        
        <h2>2. The Implementation (Word-Based Overlap)</h2>
        <p>The logic is contained within the <strong>create_chunks</strong> function in <strong>yil_gpt/ingestion.py</strong>. Since your system uses a word-based splitter (<code>words = text.split()</code>), the overlap is calculated in terms of words, not characters.</p>
        
        <ol>
            <li><strong>Current Chunk Definition:</strong> The first chunk starts at <code>start = 0</code> and ends at <code>end = start + CHUNK_SIZE</code>.</li>
            <li><strong>Next Chunk Calculation:</strong> After creating the current chunk, the code determines the starting point for the next chunk. Instead of starting immediately where the previous one ended (<code>start + CHUNK_SIZE</code>), the new start position is backed up by the <code>CHUNK_OVERLAP</code>.</li>
        </ol>
        
        <pre><code># From create_chunks in ingestion.py
# ... create current chunk ...

# Calculate the next starting point with overlap
# This advances the start position by (50 - 10) = 40 words
start += max(CHUNK_SIZE - CHUNK_OVERLAP, 1)</code></pre>
        
        <h3>Example Walkthrough</h3>
        <p>If you have a document split into 50-word chunks with a 10-word overlap:</p>
        
        <div class="config-table">
            <table>
                <thead>
                    <tr>
                        <th>Chunk</th>
                        <th>Words Included</th>
                        <th>Overlap</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Chunk 1</strong></td>
                        <td>Words 1 through 50</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td><strong>Chunk 2</strong></td>
                        <td>Words 41 through 90</td>
                        <td>Words 41‚Äì50 are repeated from Chunk 1.</td>
                    </tr>
                    <tr>
                        <td><strong>Chunk 3</strong></td>
                        <td>Words 81 through 130</td>
                        <td>Words 81‚Äì90 are repeated from Chunk 2.</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="highlight">
            This strategy ensures that any semantic idea or entity that spans the break between chunks will be fully contained within at least one chunk (or partially within two), giving the embedding model sufficient context to capture its meaning and ensuring the LLM has all the necessary text to generate an accurate response.
        </div>
        
        <!-- Question 6 -->
        <div class="question">
            ‚ùì Question 6: How did you handle the images inside the PDF to make them searchable via text?
        </div>
        
        <div class="warning">
            <strong>‚ö†Ô∏è Current Limitation:</strong> The system focuses exclusively on the searchable text layer of the PDFs.
        </div>
        
        <p>Here is a breakdown of why images are not searchable and what would be required to implement that capability:</p>
        
        <h2>1. Current Approach: Text-Only Extraction</h2>
        <p>Your pipeline uses the <strong>PyPDFLoader</strong> from langchain_community to ingest documents.</p>
        
        <ul>
            <li><strong>Text Layer Dependency:</strong> This loader primarily extracts the raw, digitized text layer from the PDF. If a document contains an image (like a chart, diagram, or a scanned image of text), the PyPDFLoader skips it unless the image contains embedded searchable text (which is rare for complex visual elements).</li>
            <li><strong>Text-Only Embedding:</strong> The pipeline then uses a text-only embedding model, <strong>"sentence-transformers/all-MiniLM-L6-v2"</strong>, which can only process the extracted text strings. It has no capability to interpret image pixels.</li>
        </ul>
        
        <h2>2. How to Make Images Searchable (Future Implementation)</h2>
        <p>To make the content of images searchable, the system would need to be upgraded to a <strong>Multimodal RAG pipeline</strong>, incorporating one of the following methods:</p>
        
        <div class="config-table">
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Process</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Optical Character Recognition (OCR)</strong></td>
                        <td>A specialized tool (e.g., Tesseract, Google Cloud Vision) scans image regions of the PDF and converts any text within images (e.g., chart labels, scanned documents) into searchable strings that can be embedded alongside the regular text.</td>
                    </tr>
                    <tr>
                        <td><strong>Visual Language Models (VLMs)</strong></td>
                        <td>For complex visuals like diagrams or flowcharts, a multimodal model (e.g., CLIP or a VLM capable of generating image descriptions) is used to create text descriptions of the visual content, which are then embedded and made searchable.</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>
</body>
</html>